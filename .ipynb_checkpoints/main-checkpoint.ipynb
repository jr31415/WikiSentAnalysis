{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3944b861-3e44-42eb-ad9e-034f7a0be026",
   "metadata": {},
   "source": [
    "**File reading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d239d6-5aaa-4d66-92b3-d0bcd1d0cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def samplestring_to_num(sampleoutput):\n",
    "    \"\"\"Takes a list (sample, output) where sample is \"Positive\", \"Negative\", \"Neutral\",\n",
    "    or \"Irrelevant\", and returns the same list, but the aforementioned terms are\n",
    "    replaced with 1, -1, 0, or None\n",
    "    \"\"\"\n",
    "    match sampleoutput[0]:\n",
    "        case \"Positive\":\n",
    "            return [0, sampleoutput[1]]\n",
    "        case \"Negative\":\n",
    "            return [1, sampleoutput[1]]\n",
    "        case \"Neutral\":\n",
    "            return [2, sampleoutput[1]]\n",
    "        case \"Irrelevant\":\n",
    "            return None\n",
    "\n",
    "def format_data(input_data):\n",
    "    \"\"\"Takes unformated data from training/validation, and formats it into a list\n",
    "\n",
    "    Input: input_data -- a string, represeting one line from twitter_training/validation\n",
    "    \"\"\"\n",
    "    row = next(csv.reader([input_data]))[2:]\n",
    "    if len(row) != 2:\n",
    "        return None\n",
    "    return samplestring_to_num(row)\n",
    "    \n",
    "def create_data(data_csv, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(data_csv, 'r') as infile:\n",
    "        training = infile.read().split(\"\\n\")[:max_samples]\n",
    "        formatted = [format_data(data) for data in training]\n",
    "        formatted = [line for line in formatted if line is not None]\n",
    "        (outputs, samples) = ([], [])\n",
    "        for data in formatted:\n",
    "            if data == None:\n",
    "                continue\n",
    "            tokens = [token.vector for token in nlp(data[1].lower()) if not token.is_space]\n",
    "            if len(tokens) == 0:\n",
    "                continue    \n",
    "            outputs.append(data[0])\n",
    "            samples.append(tokens)\n",
    "\n",
    "    return (samples, outputs)\n",
    "\n",
    "def create_model(training_data, output_location, input_size = 300, hidden_size = 64, num_layers = 2, num_outputs = 3,\\\n",
    "                 epochs = 96, batch_size = 32, preexisting_model = None, max_samples = None):\n",
    "    \"\"\"Creates a model and saves it to a file\n",
    "\n",
    "    Inputs:\n",
    "    training_data -- a string, directory of a csv of training data\n",
    "    output_location -- a string, directory of where model is saved\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \"\"\"\n",
    "    if training_data.split(\".\")[-1] not in [\"csv\", \"csv/\"]:\n",
    "        raise TypeError(\"training_data should be a csv file.\")\n",
    "   \n",
    "    (samples, outputs) = create_data(training_data, max_samples)\n",
    "\n",
    "    tensor_samples = [torch.tensor(sample, dtype=torch.float32) for sample in samples]\n",
    "    dataset = list(zip(tensor_samples, outputs))\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    sampleslen = len(samples)\n",
    "    if preexisting_model == None:\n",
    "        model = BiRNN(input_size, hidden_size, num_layers, num_outputs)\n",
    "    else:\n",
    "        with open(preexisting_model, 'rb') as infile:\n",
    "            model = pickle.load(infile)\n",
    "    \n",
    "    model = train(model, dataloader, epochs, sampleslen, \"cpu\")\n",
    "    \n",
    "    print(format(f\"Writing to {output_location}...\"))\n",
    "    with open(output_location, 'wb') as outfile:\n",
    "        pickle.dump(model, outfile)\n",
    "    print(\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027f5512-bb88-4474-b479-f9fe68f74c3b",
   "metadata": {},
   "source": [
    "**Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942d6c51-526f-4a36-976e-0fe2976e8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 2, num_outputs= 3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        self.bidirectional = True\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, bidirectional = True, dropout=0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_out = nn.Linear(64, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, padded_sequences, lengths):\n",
    "        packed = pack_padded_sequence(padded_sequences, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, dummy = pad_packed_sequence(output, batch_first=True)\n",
    "        final_output = output[torch.arange(output.size(0)), lengths - 1]\n",
    "\n",
    "        out = self.relu(self.fc1(final_output))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc_out(out)\n",
    "        \n",
    "model = BiRNN(input_size=300, hidden_size=48, num_layers=2, num_outputs=3)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    samples, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(sample) for sample in samples])\n",
    "    samples_padded = pad_sequence(samples, batch_first=True)\n",
    "    lengths, perm_idx = lengths.sort(descending=True)\n",
    "    samples_padded = samples_padded[perm_idx]\n",
    "    labels = torch.tensor(labels)[perm_idx]\n",
    "    return samples_padded, lengths, labels\n",
    "\n",
    "def train(model, dataloader, epochs, samples, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        overallloss = 0 \n",
    "        for sampleid, (padded_batch, lengths, labels) in enumerate(dataloader):\n",
    "            if sampleid == 0 and epoch == 0:\n",
    "                timea = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            model_output = model(padded_batch, lengths)\n",
    "            loss = loss_func(model_output, labels)\n",
    "            overallloss += loss.item() * labels.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if sampleid == 10 and epoch == 0:\n",
    "                overalltime = time.time() - timea\n",
    "                print(format(f\"ETA: {(overalltime * (len(dataloader)/10)) * epochs} seconds\"))\n",
    "        if epoch % 5 == 0:\n",
    "            print(format(f\"Epoch {epoch}: Loss = {overallloss/samples}\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "461c3e43-5b04-4182-b30c-c463ae8181ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 1886.536033630371 seconds\n",
      "Epoch 0: Loss = 0.3456712493533747\n",
      "Epoch 5: Loss = 0.33707948032315627\n",
      "Epoch 10: Loss = 0.31879661679815235\n",
      "Epoch 15: Loss = 0.3136616826862823\n",
      "Epoch 20: Loss = 0.3013422507175108\n",
      "Epoch 25: Loss = 0.2908162287408661\n",
      "Epoch 30: Loss = 0.29530965123704456\n",
      "Epoch 35: Loss = 0.27078714738059173\n",
      "Epoch 40: Loss = 0.2665974939253795\n",
      "Epoch 45: Loss = 0.2630106642600872\n",
      "Writing to models/twittermodel144epochs.model...\n",
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "create_model(\"archive/twitter_training.csv\", \"models/twittermodel144epochs.model\", preexisting_model=\"models/twittermodel9.model\", epochs = 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e543ce6-ceb0-4cbb-96e8-a75e2287f342",
   "metadata": {},
   "source": [
    "**Model Validation & Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efd2fb7c-13cc-4602-8429-239561a62c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validation_samples, max_samples=None):\n",
    "    with open(model, 'rb') as infile:\n",
    "        model = pickle.load(infile)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    overallloss = 0\n",
    "    acc = 0\n",
    "    (samples, outputs) = create_data(validation_samples, max_samples)\n",
    "    for (sample, output) in zip(samples, outputs):\n",
    "        out = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)\n",
    "        modeloutput = model(out, torch.tensor([out.size(1)]))\n",
    "        pred = torch.argmax(modeloutput, dim=1).item()\n",
    "        if pred == output:\n",
    "            acc += 1\n",
    "    return acc/len(samples)   \n",
    "\n",
    "def test(sample, model):\n",
    "    sample = [token.vector for token in nlp(sample.lower()) if not token.is_space]\n",
    "    tensor_sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)\n",
    "    modeloutput = model(tensor_sample, torch.tensor([tensor_sample.size(1)]))\n",
    "    match torch.argmax(modeloutput, dim=1).item():\n",
    "        case 0:\n",
    "            return 1\n",
    "        case 1:\n",
    "            return 0\n",
    "        case 2:\n",
    "            return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eaafb298-7529-4ed3-b025-d2a338d64854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Contents': 0.46153846153846156,\n",
       " 'Etymology': 0.375,\n",
       " 'Description': 0.3974358974358974,\n",
       " 'Taxonomy': 0.6428571428571429,\n",
       " 'Cultivation': 0.5492424242424242,\n",
       " 'Toxicity': 0.4166666666666667,\n",
       " 'Uses': 0.3448275862068966,\n",
       " 'In culture': 0.4883720930232558}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sections(txt_file):\n",
    "    \"\"\"Takes in a specially formatted txt file\n",
    "    containing a wikipedia article, and returns\n",
    "    the text from each section\n",
    "    \"\"\"\n",
    "    sections = dict()\n",
    "    with open(txt_file, 'r') as infile:\n",
    "        sections_list = infile.read().split(\"_____\")\n",
    "\n",
    "    topics = [section.split(\"\\n\")[0] for section in sections_list if section.split() != \"\"]\n",
    "\n",
    "    for topic_id, topic in enumerate(topics):\n",
    "        sections[topic] = sections_list[topic_id][len(topic):]\n",
    "    \n",
    "    del sections[\"External links\"]\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def cleanup_section(text):\n",
    "    \"\"\"Takes an unformatted section of text,\n",
    "    and cleans it up, removing any bracketed numbers\n",
    "    and newlines\n",
    "    \"\"\"\n",
    "    chars = \"/()\\\"'$*\"\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)\n",
    "    for char in chars:\n",
    "        text = text.replace(char, \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "def get_averages(txt_file, model):\n",
    "    outputs = dict()\n",
    "    sections = get_sections(txt_file)\n",
    "\n",
    "    for section in sections.items():\n",
    "        cleaned = cleanup_section(section[1])\n",
    "        if cleaned == \"\":\n",
    "            continue\n",
    "        \n",
    "        outputs[section[0]] = [test(sample, model) for sample in cleanup_section(section[1]).split(\".\") if sample.replace(\" \", \"\") != \"\"]\n",
    "        outputs[section[0]] = sum(outputs[section[0]])/len(outputs[section[0]])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "with open(\"models/twittermodel144epochs.model\", 'rb') as infile:\n",
    "        model = pickle.load(infile)\n",
    "get_averages(\"wikipedia articles/apples\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77f4d3-4b63-43d2-bc8e-35076f623a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53436e-01a1-422c-b334-cdff45656783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
