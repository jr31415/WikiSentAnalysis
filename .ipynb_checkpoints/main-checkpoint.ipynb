{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d239d6-5aaa-4d66-92b3-d0bcd1d0cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def samplestring_to_num(sampleoutput):\n",
    "    \"\"\"Takes a list (sample, output) where sample is \"Positive\", \"Negative\", \"Neutral\",\n",
    "    or \"Irrelevant\", and returns the same list, but the aforementioned terms are\n",
    "    replaced with 1, -1, 0, or None\n",
    "    \"\"\"\n",
    "    match sampleoutput[0]:\n",
    "        case \"Positive\":\n",
    "            return [0, sampleoutput[1]]\n",
    "        case \"Negative\":\n",
    "            return [1, sampleoutput[1]]\n",
    "        case \"Neutral\":\n",
    "            return [2, sampleoutput[1]]\n",
    "        case \"Irrelevant\":\n",
    "            return None\n",
    "\n",
    "def format_data(input_data):\n",
    "    \"\"\"Takes unformated data from training/validation, and formats it into a list\n",
    "\n",
    "    Input: input_data -- a string, represeting one line from twitter_training/validation\n",
    "    \"\"\"\n",
    "    row = next(csv.reader([input_data]))[2:]\n",
    "    if len(row) != 2:\n",
    "        return None\n",
    "    return samplestring_to_num(row)\n",
    "    \n",
    "def create_data(data_csv, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(data_csv, 'r') as infile:\n",
    "        training = infile.read().split(\"\\n\")[:max_samples]\n",
    "        formatted = [format_data(data) for data in training]\n",
    "        formatted = [line for line in formatted if line is not None]\n",
    "        (outputs, samples) = ([], [])\n",
    "        for data in formatted:\n",
    "            if data == None:\n",
    "                continue\n",
    "            tokens = [token.vector for token in nlp(data[1].lower()) if not token.is_space]\n",
    "            if len(tokens) == 0:\n",
    "                continue    \n",
    "            outputs.append(data[0])\n",
    "            samples.append(tokens)\n",
    "\n",
    "    return (samples, outputs)\n",
    "\n",
    "def create_data_sst(snippets, labels, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation from the sst dataset\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(snippets, 'r') as infile:\n",
    "        sentences = infile.read().split(\"\\n\")[:max_samples]\n",
    "    samples = [[token.vector for token in nlp(sentence.lower()) if not token.is_space]\\\n",
    "                for sentence in sentences if sentence != None]\n",
    "    with open(labels, 'r') as infile:\n",
    "        classes = infile.read()[1:max_samples+1]\n",
    "    outputs = [float(re.match(r\"..\\|(.*)\", val) for val in classes if val != None)]\n",
    "\n",
    "    return (samples, outputs)\n",
    "\n",
    "def create_model(training_data, output_location, input_size = 300, hidden_size = 64, num_layers = 2, num_outputs = 3,\\\n",
    "                 epochs = 96, batch_size = 32, preexisting_model = None, max_samples = None):\n",
    "    \"\"\"Creates a model and saves it to a file\n",
    "\n",
    "    Inputs:\n",
    "    training_data -- a string, directory of a csv of training data\n",
    "    output_location -- a string, directory of where model is saved\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \"\"\"\n",
    "    if training_data.split(\".\")[-1] not in [\"csv\", \"csv/\"]:\n",
    "        raise TypeError(\"training_data should be a csv file.\")\n",
    "   \n",
    "    (samples, outputs) = create_data(training_data, max_samples)\n",
    "\n",
    "    tensor_samples = [torch.tensor(sample, dtype=torch.float32) for sample in samples]\n",
    "    dataset = list(zip(tensor_samples, outputs))\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    sampleslen = len(samples)\n",
    "    if preexisting_model == None:\n",
    "        model = BiRNN(input_size, hidden_size, num_layers, num_outputs)\n",
    "    else:\n",
    "        with open(preexisting_model, 'rb') as infile:\n",
    "            model = pickle.load(infile)\n",
    "    \n",
    "    model = train(model, dataloader, epochs, sampleslen, \"cpu\")\n",
    "    \n",
    "    print(format(f\"Writing to {output_location}...\"))\n",
    "    with open(output_location, 'wb') as outfile:\n",
    "        pickle.dump(model, outfile)\n",
    "    print(\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "942d6c51-526f-4a36-976e-0fe2976e8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 2, num_outputs= 3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        self.bidirectional = True\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, bidirectional = True, dropout=0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc_out = nn.Linear(64, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, padded_sequences, lengths):\n",
    "        packed = pack_padded_sequence(padded_sequences, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, dummy = pad_packed_sequence(output, batch_first=True)\n",
    "        final_output = output[torch.arange(output.size(0)), lengths - 1]\n",
    "\n",
    "        out = self.relu(self.fc1(final_output))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc_out(out)\n",
    "        \n",
    "model = BiRNN(input_size=300, hidden_size=48, num_layers=2, num_outputs=3)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    samples, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(sample) for sample in samples])\n",
    "    samples_padded = pad_sequence(samples, batch_first=True)\n",
    "    lengths, perm_idx = lengths.sort(descending=True)\n",
    "    samples_padded = samples_padded[perm_idx]\n",
    "    labels = torch.tensor(labels)[perm_idx]\n",
    "    return samples_padded, lengths, labels\n",
    "\n",
    "def train(model, dataloader, epochs, samples, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        overallloss = 0 \n",
    "        for sampleid, (padded_batch, lengths, labels) in enumerate(dataloader):\n",
    "            if sampleid == 0 and epoch == 0:\n",
    "                timea = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            model_output = model(padded_batch, lengths)\n",
    "            loss = loss_func(model_output, labels)\n",
    "            overallloss += loss.item() * labels.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if sampleid == 10 and epoch == 0:\n",
    "                overalltime = time.time() - timea\n",
    "                print(format(f\"ETA: {(overalltime * (len(dataloader)/10)) * epochs} seconds\"))\n",
    "        if epoch % 5 == 0:\n",
    "            print(format(f\"Epoch {epoch}: Loss = {overallloss/samples}\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "461c3e43-5b04-4182-b30c-c463ae8181ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 1886.536033630371 seconds\n",
      "Epoch 0: Loss = 0.3456712493533747\n",
      "Epoch 5: Loss = 0.33707948032315627\n",
      "Epoch 10: Loss = 0.31879661679815235\n",
      "Epoch 15: Loss = 0.3136616826862823\n",
      "Epoch 20: Loss = 0.3013422507175108\n",
      "Epoch 25: Loss = 0.2908162287408661\n",
      "Epoch 30: Loss = 0.29530965123704456\n",
      "Epoch 35: Loss = 0.27078714738059173\n",
      "Epoch 40: Loss = 0.2665974939253795\n",
      "Epoch 45: Loss = 0.2630106642600872\n",
      "Writing to models/twittermodel144epochs.model...\n",
      "Model created successfully!\n"
     ]
    }
   ],
   "source": [
    "create_model(\"archive/twitter_training.csv\", \"models/twittermodel144epochs.model\", preexisting_model=\"models/twittermodel9.model\", epochs = 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "efd2fb7c-13cc-4602-8429-239561a62c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8490338164251208"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_model(model, validation_samples, max_samples=None):\n",
    "    with open(model, 'rb') as infile:\n",
    "        model = pickle.load(infile)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    overallloss = 0\n",
    "    acc = 0\n",
    "    (samples, outputs) = create_data(validation_samples, max_samples)\n",
    "    for (sample, output) in zip(samples, outputs):\n",
    "        out = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)\n",
    "        modeloutput = model(out, torch.tensor([out.size(1)]))\n",
    "        pred = torch.argmax(modeloutput, dim=1).item()\n",
    "        if pred == output:\n",
    "            acc += 1\n",
    "    return acc/len(samples)   \n",
    "\n",
    "validate_model(\"models/twittermodel144epochs.model\", \"archive/twitter_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "4c4fae1c-5722-4d89-aca9-ca191a25e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "sample = [token.vector for token in nlp(text.lower()) if not token.is_space]\n",
    "tensor_sample = torch.tensor(sample, dtype=torch.float32).unsqueeze(0)\n",
    "modeloutput = model(tensor_sample, torch.tensor([tensor_sample.size(1)]))\n",
    "torch.argmax(modeloutput, dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "897e5613-f09b-4653-ac4f-3b9b61e774bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_data_sst(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/original_rt_snippets.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mcreate_data_sst\u001b[0;34m(snippets, labels, max_samples)\u001b[0m\n\u001b[1;32m     77\u001b[0m samples \u001b[38;5;241m=\u001b[39m [[token\u001b[38;5;241m.\u001b[39mvector \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m nlp(sentence\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_space]\\\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m sentence \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m infile:\n\u001b[0;32m---> 80\u001b[0m     classes \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mread()[\u001b[38;5;241m1\u001b[39m:max_samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     81\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m, val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m classes \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (samples, outputs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "create_data_sst(\"SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/original_rt_snippets.txt\", \"SST2-Data/stanfordSentimentTreebank/stanfordSentimentTreebank/sentiment_labels.txt\", max_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a80c7e-7a40-459d-b327-be49214626e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
