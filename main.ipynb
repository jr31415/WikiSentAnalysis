{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d3d239d6-5aaa-4d66-92b3-d0bcd1d0cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def samplestring_to_num(sampleoutput):\n",
    "    \"\"\"Takes a list (sample, output) where sample is \"Positive\", \"Negative\", \"Neutral\",\n",
    "    or \"Irrelevant\", and returns the same list, but the aforementioned terms are\n",
    "    replaced with 1, -1, 0, or None\n",
    "    \"\"\"\n",
    "    match sampleoutput[0]:\n",
    "        case \"Positive\":\n",
    "            return [0, sampleoutput[1]]\n",
    "        case \"Negative\":\n",
    "            return [1, sampleoutput[1]]\n",
    "        case \"Neutral\":\n",
    "            return [2, sampleoutput[1]]\n",
    "        case \"Irrelevant\":\n",
    "            return None\n",
    "\n",
    "def format_data(input_data):\n",
    "    \"\"\"Takes unformated data from training/validation, and formats it into a list\n",
    "\n",
    "    Input: input_data -- a string, represeting one line from twitter_training/validation\n",
    "    \"\"\"\n",
    "    row = next(csv.reader([input_data]))[2:]\n",
    "    if len(row) != 2:\n",
    "        return None\n",
    "    return samplestring_to_num(row)\n",
    "    \n",
    "def create_data(data_csv, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(data_csv, 'r') as infile:\n",
    "        training = infile.read().split(\"\\n\")[:max_samples]\n",
    "        formatted = [format_data(data) for data in training]\n",
    "        formatted = [line for line in formatted if line is not None]\n",
    "        (outputs, samples) = ([], [])\n",
    "        for data in formatted:\n",
    "            if data == None:\n",
    "                continue\n",
    "            tokens = [token.vector for token in nlp(data[1]) if not token.is_space]\n",
    "            if len(tokens) == 0:\n",
    "                continue    \n",
    "            outputs.append(data[0])\n",
    "            samples.append(tokens)\n",
    "\n",
    "    return (samples, outputs)\n",
    "\n",
    "def create_model(training_data, output_location, input_size = 300, hidden_size = 48, num_layers = 1, num_outputs = 3,\\\n",
    "                 epochs = 64, max_samples = None):\n",
    "    \"\"\"Creates a model and saves it to a file\n",
    "\n",
    "    Inputs:\n",
    "    training_data -- a string, directory of a csv of training data\n",
    "    output_location -- a string, directory of where model is saved\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \"\"\"\n",
    "    if training_data.split(\".\")[-1] not in [\"csv\", \"csv/\"]:\n",
    "        raise TypeError(\"training_data should be a csv file.\")\n",
    "   \n",
    "    (samples, outputs) = create_data(training_data, max_samples)\n",
    "\n",
    "    tensor_samples = [torch.tensor(sample, dtype=torch.float32) for sample in samples]\n",
    "    dataset = list(zip(tensor_samples, outputs))\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    sampleslen = len(samples)\n",
    "    model = BiRNN(input_size, hidden_size, num_layers, num_outputs)\n",
    "    model = train(model, dataloader, epochs, sampleslen, \"cpu\")\n",
    "    \n",
    "    print(format(f\"Writing to {output_location}...\"))\n",
    "    with open(output_location, 'wb') as outfile:\n",
    "        pickle.dump(model, outfile)\n",
    "    print(\"Model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "942d6c51-526f-4a36-976e-0fe2976e8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1, num_outputs= 3 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        self.bidirectional = True\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers = 1, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_outputs)\n",
    "\n",
    "    def forward(self, padded_sequences, lengths):\n",
    "        packed = pack_padded_sequence(padded_sequences, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, dummy = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        return self.fc(output[torch.arange(output.size(0)), lengths - 1])\n",
    "        \n",
    "model = BiRNN(input_size=300, hidden_size=32, num_layers=1, num_outputs=3)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    samples, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(sample) for sample in samples])\n",
    "    samples_padded = pad_sequence(samples, batch_first=True)\n",
    "    lengths, perm_idx = lengths.sort(descending=True)\n",
    "    samples_padded = samples_padded[perm_idx]\n",
    "    labels = torch.tensor(labels)[perm_idx]\n",
    "    return samples_padded, lengths, labels\n",
    "\n",
    "def train(model, dataloader, epochs, samples, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        overallloss = 0 \n",
    "        for sampleid, (padded_batch, lengths, labels) in enumerate(dataloader):\n",
    "            if sampleid == 1 and epoch == 0:\n",
    "                timea = time.time()\n",
    "            optimizer.zero_grad() #Clear previous gradients\n",
    "            model_output = model(padded_batch, lengths)\n",
    "            loss = loss_func(model_output, labels)\n",
    "            overallloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if sampleid == 100 and epoch == 0:\n",
    "                overalltime = time.time() - timea\n",
    "                print(format(f\"ETA: {(overalltime * (samples/100)) * epochs} seconds\"))\n",
    "        if epoch % 5 == 0:\n",
    "            print(format(f\"Epoch {epoch}: Loss = {overallloss/samples}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "#model = train(model, samples, outputs, 100, \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "461c3e43-5b04-4182-b30c-c463ae8181ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchive/twitter_training.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/twittermodel.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[145], line 76\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(training_data, output_location, input_size, hidden_size, num_layers, num_outputs, epochs, max_samples)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv/\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data should be a csv file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m (samples, outputs) \u001b[38;5;241m=\u001b[39m create_data(training_data, max_samples)\n\u001b[1;32m     78\u001b[0m tensor_samples \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mtensor(sample, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples]\n\u001b[1;32m     79\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(tensor_samples, outputs))\n",
      "Cell \u001b[0;32mIn[145], line 56\u001b[0m, in \u001b[0;36mcreate_data\u001b[0;34m(data_csv, max_samples)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mvector \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m nlp(data[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_space]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/language.py:1053\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/spacy/pipeline/tok2vec.py:121\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 121\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(docs)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m lengths \u001b[38;5;241m=\u001b[39m NUMPY_OPS\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     80\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](X, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_model(\"archive/twitter_training.csv\", \"models/twittermodel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d1154787-ae9a-4054-a291-54b69da753ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.4839, -0.2181, -1.0030]])\n",
      "Probabilities: tensor([[0.5808, 0.2879, 0.1313]])\n",
      "Predicted class index: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/twittermodel.model\", 'rb') as infile:\n",
    "    model = pickle.load(infile)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    vectors = [token.vector for token in doc if token.has_vector]\n",
    "    return torch.tensor(vectors, dtype=torch.float32).unsqueeze(0)  # shape: (1, seq_len, embedding_dim)\n",
    "\n",
    "input_text = \"\"\"This is a big ball\"\"\"\n",
    "input_tensor = preprocess_text(input_text)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor, torch.tensor([input_tensor.size(1)]))\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fae1c-5722-4d89-aca9-ca191a25e262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
