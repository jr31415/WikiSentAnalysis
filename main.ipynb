{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d3d239d6-5aaa-4d66-92b3-d0bcd1d0cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def samplestring_to_num(sampleoutput):\n",
    "    \"\"\"Takes a list (sample, output) where sample is \"Positive\", \"Negative\", \"Neutral\",\n",
    "    or \"Irrelevant\", and returns the same list, but the aforementioned terms are\n",
    "    replaced with 1, -1, 0, or None\n",
    "    \"\"\"\n",
    "    match sampleoutput[0]:\n",
    "        case \"Positive\":\n",
    "            return [0, sampleoutput[1]]\n",
    "        case \"Negative\":\n",
    "            return [1, sampleoutput[1]]\n",
    "        case \"Neutral\":\n",
    "            return [2, sampleoutput[1]]\n",
    "        case \"Irrelevant\":\n",
    "            return None\n",
    "\n",
    "def format_data(input_data):\n",
    "    \"\"\"Takes unformated data from training/validation, and formats it into a list\n",
    "\n",
    "    Input: input_data -- a string, represeting one line from twitter_training/validation\n",
    "    \"\"\"\n",
    "    data = next(csv.reader([input_data]))[2:]\n",
    "    return samplestring_to_num(data)\n",
    "    \n",
    "def create_data(data_csv, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(\"archive/twitter_training.csv\", 'r') as infile:\n",
    "        training = infile.read().split(\"\\n\")[:max_samples]\n",
    "        formatted = [format_data(data) for data in training]\n",
    "\n",
    "        (outputs, samples) = ([], [])\n",
    "        for data in formatted:\n",
    "            if data == None:\n",
    "                continue\n",
    "            tokens = [token.vector for token in nlp(data[1]) if not token.is_space]\n",
    "            if len(tokens) == 0:\n",
    "                continue    \n",
    "            outputs.append(data[0])\n",
    "            samples.append(tokens)\n",
    "\n",
    "        \n",
    "\n",
    "    return (samples, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "942d6c51-526f-4a36-976e-0fe2976e8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, outputs = create_data(\"archive/twitter_training.csv\", max_samples = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "791e58aa-04cb-47d9-a6de-4dd69e1ba842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.0710766715120201\n",
      "ETA: 66.01144409179686 seconds\n",
      "Epoch 5: Loss = 0.06395531084087837\n",
      "Epoch 10: Loss = 0.0033793560277564803\n",
      "Epoch 15: Loss = 0.0009574272833647445\n",
      "Epoch 20: Loss = 0.00033566628063512715\n",
      "Epoch 25: Loss = 0.00012603269680521126\n",
      "Epoch 30: Loss = 4.876349944929618e-05\n",
      "Epoch 35: Loss = 1.9516887287253786e-05\n",
      "Epoch 40: Loss = 0.001310447652724072\n",
      "Epoch 45: Loss = 0.0004050917894689014\n",
      "Epoch 50: Loss = 0.00016904721634714451\n",
      "Epoch 55: Loss = 7.404154975286348e-05\n",
      "Epoch 60: Loss = 3.275602297004614e-05\n",
      "Epoch 65: Loss = 1.4265907269373775e-05\n",
      "Epoch 70: Loss = 6.075912933473326e-06\n",
      "Epoch 75: Loss = 2.5464550421584225e-06\n",
      "Epoch 80: Loss = 1.0561849603435502e-06\n",
      "Epoch 85: Loss = 4.2775054791592396e-07\n",
      "Epoch 90: Loss = 1.6929715948211652e-07\n",
      "Epoch 95: Loss = 6.210903076747172e-08\n"
     ]
    }
   ],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1, num_outputs= 3 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        self.bidirectional = True\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers = 1, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lengths = torch.tensor([len(seq) for seq in x]) #Get lengths of inputs\n",
    "        lengths, permx = lengths.sort(descending = True) #Sort inputs\n",
    "        padded_sequence = pad_sequence(x, batch_first = True)\n",
    "        padded_sequence = padded_sequence[permx]\n",
    "        packed_sequence = pack_padded_sequence(padded_sequence, lengths, batch_first = True)\n",
    "        output, hidden = self.rnn(packed_sequence)\n",
    "        output, dummy = pad_packed_sequence(output, batch_first = True)\n",
    "        batch_size = output.size(0)\n",
    "        last_timesteps = lengths - 1\n",
    "        batch_indicies = torch.arange(batch_size)\n",
    "        \n",
    "        return self.fc(output[batch_indicies, last_timesteps, :])\n",
    "    \n",
    "\n",
    "        \n",
    "model = BiRNN(input_size=300, hidden_size=32, num_layers=1, num_outputs=3)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, samples, outputs, epochs, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sampleslen = len(samples)\n",
    "        overallloss = 0 \n",
    "        for sampleid, (sample, actual) in enumerate(zip(samples, outputs)):\n",
    "            if sampleid == 1 and epoch == 1:\n",
    "                timea = time.time()\n",
    "            optimizer.zero_grad() #Clear previous gradients\n",
    "            model_output = model(torch.tensor([sample], dtype=torch.float32))\n",
    "            loss = loss_func(model_output, torch.tensor(actual).unsqueeze(0))\n",
    "            overallloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if sampleid == 100 and epoch == 1:\n",
    "                overalltime = time.time() - timea\n",
    "                print(format(f\"ETA: {(overalltime * (sampleslen/100)) * epochs} seconds\"))\n",
    "        if epoch % 5 == 0:\n",
    "            print(format(f\"Epoch {epoch}: Loss = {overallloss/sampleslen}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(model, samples, outputs, 100, \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "fbb0a187-6833-4c82-a351-76f58a87f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-10.1431,  -1.0854,  10.3965]])\n",
      "Probabilities: tensor([[1.2017e-09, 1.0315e-05, 9.9999e-01]])\n",
      "Predicted class index: 2\n"
     ]
    }
   ],
   "source": [
    "input_text = \"For some reason, my laptop can run Borderlands 3, War thunder and Warframe all on high settings.. . BUT. . The moment I even try to even launch Csgo my entire computer crashes...\"\n",
    "input_tensor = preprocess_text(input_text)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc5973-c288-49b6-a9b6-f45793df3658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
