{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d3d239d6-5aaa-4d66-92b3-d0bcd1d0cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 7747.9317086792 seconds\n",
      "Epoch 0: Loss = 0.9697057217228959\n",
      "Epoch 5: Loss = 0.896549934248851\n",
      "Epoch 10: Loss = 0.8635651615234579\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(model, outfile)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel created successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m create_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchive/twitter_training.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/twittermodel.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[216], line 78\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(training_data, output_location, input_size, hidden_size, num_layers, num_outputs, epochs, max_samples)\u001b[0m\n\u001b[1;32m     76\u001b[0m (samples, outputs) \u001b[38;5;241m=\u001b[39m create_data(training_data, max_samples)\n\u001b[1;32m     77\u001b[0m model \u001b[38;5;241m=\u001b[39m BiRNN(input_size, hidden_size, num_layers, num_outputs)\n\u001b[0;32m---> 78\u001b[0m model \u001b[38;5;241m=\u001b[39m train(model, samples, outputs, epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mformat\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_location, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n",
      "Cell \u001b[0;32mIn[214], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, samples, outputs, epochs, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m#Clear previous gradients\u001b[39;00m\n\u001b[1;32m     43\u001b[0m model_output \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor([sample], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(model_output, torch\u001b[38;5;241m.\u001b[39mtensor(actual)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     45\u001b[0m overallloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:1297\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1299\u001b[0m         target,\n\u001b[1;32m   1300\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1301\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[1;32m   1302\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1303\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[1;32m   1304\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[1;32m   3495\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3496\u001b[0m     target,\n\u001b[1;32m   3497\u001b[0m     weight,\n\u001b[1;32m   3498\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[1;32m   3499\u001b[0m     ignore_index,\n\u001b[1;32m   3500\u001b[0m     label_smoothing,\n\u001b[1;32m   3501\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import pickle\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def samplestring_to_num(sampleoutput):\n",
    "    \"\"\"Takes a list (sample, output) where sample is \"Positive\", \"Negative\", \"Neutral\",\n",
    "    or \"Irrelevant\", and returns the same list, but the aforementioned terms are\n",
    "    replaced with 1, -1, 0, or None\n",
    "    \"\"\"\n",
    "    match sampleoutput[0]:\n",
    "        case \"Positive\":\n",
    "            return [0, sampleoutput[1]]\n",
    "        case \"Negative\":\n",
    "            return [1, sampleoutput[1]]\n",
    "        case \"Neutral\":\n",
    "            return [2, sampleoutput[1]]\n",
    "        case \"Irrelevant\":\n",
    "            return None\n",
    "\n",
    "def format_data(input_data):\n",
    "    \"\"\"Takes unformated data from training/validation, and formats it into a list\n",
    "\n",
    "    Input: input_data -- a string, represeting one line from twitter_training/validation\n",
    "    \"\"\"\n",
    "    row = next(csv.reader([input_data]))[2:]\n",
    "    if len(row) != 2:\n",
    "        return None\n",
    "    return samplestring_to_num(row)\n",
    "    \n",
    "def create_data(data_csv, max_samples=None):\n",
    "    \"\"\"Create data formatted for training and validation\n",
    "    \n",
    "    Inputs:\n",
    "    data_csv -- a string, directory of a csv of training/validation data\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \n",
    "    Output: (samples, outputs) samples -- a list of vectorized data, outputs -- the model output given\n",
    "    the vectorized data\n",
    "    \"\"\"\n",
    "    with open(data_csv, 'r') as infile:\n",
    "        training = infile.read().split(\"\\n\")[:max_samples]\n",
    "        formatted = [format_data(data) for data in training]\n",
    "        formatted = [line for line in formatted if line is not None]\n",
    "        (outputs, samples) = ([], [])\n",
    "        for data in formatted:\n",
    "            if data == None:\n",
    "                continue\n",
    "            tokens = [token.vector for token in nlp(data[1]) if not token.is_space]\n",
    "            if len(tokens) == 0:\n",
    "                continue    \n",
    "            outputs.append(data[0])\n",
    "            samples.append(tokens)\n",
    "\n",
    "    return (samples, outputs)\n",
    "\n",
    "def create_model(training_data, output_location, input_size = 300, hidden_size = 48, num_layers = 1, num_outputs = 3,\\\n",
    "                 epochs = 64, max_samples = None):\n",
    "    \"\"\"Creates a model and saves it to a file\n",
    "\n",
    "    Inputs:\n",
    "    training_data -- a string, directory of a csv of training data\n",
    "    output_location -- a string, directory of where model is saved\n",
    "    max_samples -- int, how many samples to include, default is all (useful for debugging)\n",
    "    \"\"\"\n",
    "    if training_data.split(\".\")[-1] not in [\"csv\", \"csv/\"]:\n",
    "        raise TypeError(\"training_data should be a csv file.\")\n",
    "   \n",
    "    (samples, outputs) = create_data(training_data, max_samples)\n",
    "    model = BiRNN(input_size, hidden_size, num_layers, num_outputs)\n",
    "    model = train(model, samples, outputs, epochs, \"cpu\")\n",
    "    \n",
    "    print(format(f\"Writing to {output_location}...\"))\n",
    "    with open(output_location, 'wb') as outfile:\n",
    "        pickle.dump(model, outfile)\n",
    "    print(\"Model created successfully!\")\n",
    "\n",
    "create_model(\"archive/twitter_training.csv\", \"models/twittermodel.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942d6c51-526f-4a36-976e-0fe2976e8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, outputs = create_data(\"archive/twitter_training.csv\", max_samples = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "791e58aa-04cb-47d9-a6de-4dd69e1ba842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1, num_outputs= 3 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_outputs = num_outputs\n",
    "        self.bidirectional = True\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers = 1, bidirectional = True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lengths = torch.tensor([len(seq) for seq in x]) #Get lengths of inputs\n",
    "        lengths, permx = lengths.sort(descending = True) #Sort inputs\n",
    "        padded_sequence = pad_sequence(x, batch_first = True)\n",
    "        padded_sequence = padded_sequence[permx]\n",
    "        packed_sequence = pack_padded_sequence(padded_sequence, lengths, batch_first = True)\n",
    "        output, hidden = self.rnn(packed_sequence)\n",
    "        output, dummy = pad_packed_sequence(output, batch_first = True)\n",
    "        batch_size = output.size(0)\n",
    "        last_timesteps = lengths - 1\n",
    "        batch_indicies = torch.arange(batch_size)\n",
    "        \n",
    "        return self.fc(output[batch_indicies, last_timesteps, :])\n",
    "        \n",
    "model = BiRNN(input_size=300, hidden_size=32, num_layers=1, num_outputs=3)\n",
    "\n",
    "\n",
    "\n",
    "def train(model, samples, outputs, epochs, device):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sampleslen = len(samples)\n",
    "        overallloss = 0 \n",
    "        for sampleid, (sample, actual) in enumerate(zip(samples, outputs)):\n",
    "            if sampleid == 1 and epoch == 0:\n",
    "                timea = time.time()\n",
    "            optimizer.zero_grad() #Clear previous gradients\n",
    "            model_output = model(torch.tensor([sample], dtype=torch.float32))\n",
    "            loss = loss_func(model_output, torch.tensor(actual).unsqueeze(0))\n",
    "            overallloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if sampleid == 100 and epoch == 0:\n",
    "                overalltime = time.time() - timea\n",
    "                print(format(f\"ETA: {(overalltime * (sampleslen/100)) * epochs} seconds\"))\n",
    "        if epoch % 5 == 0:\n",
    "            print(format(f\"Epoch {epoch}: Loss = {overallloss/sampleslen}\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "#model = train(model, samples, outputs, 100, \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fbb0a187-6833-4c82-a351-76f58a87f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[0.7277, 0.1725, 0.5145]])\n",
      "Probabilities: tensor([[0.4198, 0.2410, 0.3392]])\n",
      "Predicted class index: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/twittermodel.model\", 'rb') as infile:\n",
    "    model = pickle.load(infile)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    vectors = [token.vector for token in doc if token.has_vector]\n",
    "    return torch.tensor(vectors, dtype=torch.float32).unsqueeze(0)  # shape: (1, seq_len, embedding_dim)\n",
    "\n",
    "input_text = \"\"\"\"\"\"\n",
    "input_tensor = preprocess_text(input_text)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tensor)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Predicted class index: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc5973-c288-49b6-a9b6-f45793df3658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
